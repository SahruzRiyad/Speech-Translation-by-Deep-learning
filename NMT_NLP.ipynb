{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,LSTM,GRU,Dense,Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input:  4349\n",
      "num samples output:  4349\n",
      "num samples input output:  4349\n"
     ]
    }
   ],
   "source": [
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "\n",
    "count = 0\n",
    "for line in open('./ben.txt'):\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    spLine = line.rstrip().split('\\t')\n",
    "    input_sentence , output = spLine[0] , spLine[1]\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentences_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentences_input)\n",
    "\n",
    "print(\"num samples input: \",len(input_sentences))\n",
    "print(\"num samples output: \",len(output_sentences))\n",
    "print(\"num samples input output: \",len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was raised in France.\n",
      "ও ফ্রান্সে বড়ো হয়েছে। <eos>\n",
      "<sos> ও ফ্রান্সে বড়ো হয়েছে।\n",
      "I'll never stop.\n",
      "আমি কখনই থামবো না। <eos>\n",
      "<sos> আমি কখনই থামবো না।\n",
      "People from Madrid are weird.\n",
      "ম্যাড্রিডের মানুষরা অদ্ভুত হয়। <eos>\n",
      "<sos> ম্যাড্রিডের মানুষরা অদ্ভুত হয়।\n",
      "I'd like to check out tomorrow morning.\n",
      "আমি কাল সকালে ঘরটা ছেরে দিতে চাই। <eos>\n",
      "<sos> আমি কাল সকালে ঘরটা ছেরে দিতে চাই।\n",
      "She hasn't come yet.\n",
      "ও এখনো আসেনি। <eos>\n",
      "<sos> ও এখনো আসেনি।\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = np.random.randint(0,4349)\n",
    "    print(input_sentences[x])\n",
    "    print(output_sentences[x])\n",
    "    print(output_sentences_inputs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Words:  1875\n",
      "Length of Longest Sequence:  19\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total Unique Words: ',len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print('Length of Longest Sequence: ',max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 3551\n",
      "Length of longest sentence in the output: 19\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Encoder Shape:  (4349, 19)\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 78]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq,maxlen=max_input_len)\n",
    "\n",
    "print('Input Encoder Shape: ',encoder_input_sequences.shape)\n",
    "print(encoder_input_sequences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_inputs['i'])\n",
    "print(word2idx_inputs['will'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (4349, 19)\n",
      "decoder_input_sequences[172]: [  2   3  51 835   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1107\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_outputs['<sos>'])\n",
    "print(word2idx_outputs['\"আপনি'])\n",
    "print(word2idx_outputs['একজন'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('./utserver/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
